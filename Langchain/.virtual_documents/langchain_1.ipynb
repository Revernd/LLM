


from langchain_huggingface import HuggingFaceEndpoint


# Set your Hugging Face API token 
huggingfacehub_api_token = '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'




# Define the LLM
llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)

# Predict the words following the text in question
question = 'Whatever you do, take care of your shoes'
output = llm.invoke(question)

print(output)






# Create a prompt template from the template string
from langchain.prompts import PromptTemplate


template = "You are an artificial intelligence assistant, answer the question. {question}"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create a chain to integrate the prompt template and LLM
llm = HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)
llm_chain = prompt | llm

question = "How does LangChain make LLM application development easier?"
print(llm_chain.invoke({"question": question}))





from langchain.memory import ChatMessageHistory


# Create the conversation history and add the first AI message
history = ChatMessageHistory()
history.add_ai_message("Hello! Ask me anything about Python programming!")

# Add the user message to the history
history.add_user_message("What is a list comprehension?")

# Add another user message and call the model
history.add_user_message("Describe the same in fewer words")



response = llm.invoke(history.messages)
print(response)





from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain


# Define a buffer memory
memory = ConversationBufferMemory(size=4)

# Define the chain for integrating the memory with the model
buffer_chain = ConversationChain(llm=llm, memory=memory)


# Invoke the chain with the inputs provided
buffer_chain.invoke("Write Python code to draw a scatter plot.")
buffer_chain.invoke("Use the Seaborn library.")





from langchain.memory import ConversationSummaryMemory


# Define a summary memory that uses an OpenAI chat model
memory = ConversationSummaryMemory(llm=HuggingFaceEndpoint(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token))


# Define the chain for integrating the memory with the model
summary_chain = ConversationChain(llm=llm, memory=memory, verbose=True)


# Invoke the chain with the inputs provided
summary_chain.invoke("Describe the relationship of the human mind with the keyboard when taking a great online class.")
summary_chain.invoke("Use an analogy to describe it.")





# Create a prompt template that takes an input activity
learning_prompt = PromptTemplate(
    input_variables=["activity"],
    template="I want to learn how to {activity}. Can you suggest how I can learn this step-by-step?"
)

# Create a prompt template that places a time constraint on the output
time_prompt = PromptTemplate(
    input_variables=["learning_plan"],
    template="I only have one week. Can you create a plan to help me hit this goal: {learning_plan}."
)




# Invoke the learning_prompt with an activity
print(learning_prompt.invoke({"activity": "play golf"}))


from langchain_core.output_parsers import StrOutputParser

# Complete the sequential chain with LCEL
seq_chain = ({"learning_plan": learning_prompt | llm | StrOutputParser()}
    | time_prompt
    | llm
    | StrOutputParser())

# Call the chain
print(seq_chain.invoke({"activity": "write a book"}))





from langchain_community.llms import HuggingFaceHub
from langchain.agents import Tool
from langchain.utilities import WikipediaAPIWrapper
from langchain_core.prompts import PromptTemplate
from langchain.agents import initialize_agent, AgentType
from langchain.memory import ConversationBufferMemory
import os


!pip install wikipedia


# Set up the Wikipedia tool
wikipedia = WikipediaAPIWrapper()
wikipedia_tool = Tool(
    name="Wikipedia",
    func=wikipedia.run,
    description="Useful for querying information from Wikipedia. Input should be a search query."
)

# Create a list of tools
tools = [wikipedia_tool]


# Initialize the ReAct agent
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    memory=memory,
   # agent_kwargs={
    #    "prefix": prompt.template
    #}
)


from langchain.agents import initialize_agent, AgentType

# Initialize the agent
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent_type=AgentType.REACT_DESCRIPTION,
    verbose=True  # Set to True if you want detailed logs
)



response = agent.invoke({"messages": [("human", "Summarize key facts about London, England.")]})
print(response)




